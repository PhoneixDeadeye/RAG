# RAG (Retrieval-Augmented Generation) Pipeline

A complete implementation of a RAG system using LangChain, ChromaDB, and Groq LLM for intelligent document querying with conversation history support.

## ğŸ¯ Overview

This project demonstrates a production-ready RAG pipeline that combines semantic search with large language models to provide accurate, context-aware answers from your documents. It features three levels of sophistication:

- **Simple RAG**: Basic retrieval and generation
- **Enhanced RAG**: Confidence scoring and source tracking
- **Advanced RAG**: Multi-turn conversations with history awareness

## âœ¨ Features

- ğŸ“„ **Multi-format Document Loading**: Text files and PDFs
- ğŸ” **Semantic Search**: Using SentenceTransformer embeddings
- ğŸ’¾ **Persistent Vector Store**: ChromaDB with cosine similarity
- ğŸ¤– **LLM Integration**: ChatGroq (Gemma2-9b-it model)
- ğŸ’¬ **Conversation History**: Context-aware multi-turn dialogues
- ğŸ“Š **Confidence Metrics**: Similarity scores and source citations
- ğŸ¨ **Professional Output**: Formatted results with emojis and structure
- ğŸ”§ **Extensible Architecture**: Easy to customize and extend

## ğŸ—ï¸ Architecture

```
Document Sources (PDF/TXT)
         â†“
    Text Splitter (Chunking)
         â†“
  Embedding Generation (SentenceTransformer)
         â†“
   Vector Store (ChromaDB)
         â†“
    RAG Retriever â†â†’ User Query
         â†“
   LLM (ChatGroq) + Context
         â†“
    Generated Answer + Sources
```

## ğŸ“‹ Requirements

- Python 3.8+
- Required packages (see `requirements.txt`):
  - langchain
  - langchain-community
  - langchain-groq
  - sentence-transformers
  - chromadb
  - numpy
  - python-dotenv
  - pymupdf (for PDF processing)

## ğŸš€ Installation

1. **Clone the repository**:
```bash
git clone https://github.com/PhoneixDeadeye/RAG.git
cd RAG
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Set up environment variables**:
Create a `.env` file in the root directory:
```env
GROQ_API_KEY=your_groq_api_key_here
```

Get your Groq API key from: https://console.groq.com/

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ document.ipynb          # Main Jupyter notebook with full implementation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ text_files/             # Input text files
â”‚   â”œâ”€â”€ pdf_files/              # Input PDF files
â”‚   â””â”€â”€ vector_store/           # Persistent ChromaDB storage
â”œâ”€â”€ src/
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ uv.lock
```

## ğŸ’» Usage

### Quick Start

1. **Open the Jupyter notebook**:
```bash
jupyter notebook notebook/document.ipynb
```

2. **Run all cells** to initialize the components

3. **Query your documents**:

#### Simple RAG
```python
answer = rag_simple("What are the key features?", rag_retriever, llm, top_k=3)
print(answer)
```

#### Enhanced RAG (with confidence scores)
```python
result = rag_enhanced(
    "What are the key features?", 
    rag_retriever, 
    llm, 
    top_k=3, 
    min_score=0.1, 
    return_contexts=True
)
print(f"Answer: {result['answer']}")
print(f"Confidence: {result['confidence']:.2%}")
```

#### Advanced RAG (with conversation history)
```python
# Initialize pipeline
advanced_rag = AdvancedRAGPipeline(rag_retriever, llm)

# First query
result1 = advanced_rag.query("What is machine learning?", top_k=5)

# Follow-up query (uses conversation history)
result2 = advanced_rag.query("How is it different from deep learning?", top_k=5)

# View conversation history
history = advanced_rag.get_history()

# Clear history when needed
advanced_rag.clear_history()
```

## ğŸ”§ Components

### 1. EmbeddingManager
Handles document embedding generation using SentenceTransformer:
- Model: `all-MiniLM-L6-v2` (384-dimensional embeddings)
- Batch processing with progress tracking

### 2. VectorStore
Manages document embeddings in ChromaDB:
- Persistent storage
- Cosine similarity metric
- Automatic collection management

### 3. RAGRetriever
Query-based document retrieval:
- Top-k similarity search
- Score threshold filtering
- Metadata preservation

### 4. AdvancedRAGPipeline
Multi-turn conversation support:
- Tracks last 3 conversation turns
- Context-aware prompt construction
- History management methods

## ğŸ“Š Example Output

```
ğŸ” RAG ENHANCED QUERY DEMO
====================================================================================================

ğŸ“‹ Query: 'Which technique is used for caching?'
âš™ï¸  Parameters: top_k=3, min_score=0.1, return_contexts=True
----------------------------------------------------------------------------------------------------

ğŸ’¡ GENERATED ANSWER:
====================================================================================================
The document mentions using Redis for caching to improve performance...
====================================================================================================

ğŸ“š SOURCES & EVIDENCE:
----------------------------------------------------------------------------------------------------

â”Œâ”€ SOURCE #1
â”‚  ğŸ“„ File: rohan_cv_aug.pdf
â”‚  ğŸ“– Page: 2
â”‚  ğŸ¯ Similarity Score: 0.8523 (85.23%)
â”‚
â”‚  ğŸ“ Preview:
â”‚  Implemented Redis caching layer to reduce database load by 40%...
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“Š RETRIEVAL METRICS:
----------------------------------------------------------------------------------------------------
âœ… Overall Confidence: 0.8523 (85.23%)
ğŸ“¦ Documents Retrieved: 3
```

## ğŸ“ Key Concepts

### Chunking Strategy
- **Chunk Size**: 500 characters
- **Chunk Overlap**: 50 characters
- **Separators**: Paragraphs â†’ Lines â†’ Spaces

### Retrieval Parameters
- **top_k**: Number of documents to retrieve (default: 3-5)
- **min_score**: Minimum similarity threshold (default: 0.2)
- **score_threshold**: Filter results by confidence

### Conversation History
- Maintains last 3 conversation turns
- Automatically included in prompt context
- Enables follow-up questions and clarifications

## ğŸ” Advanced Features

### Custom Document Processing
```python
# Add your own documents
from langchain_community.document_loaders import DirectoryLoader

loader = DirectoryLoader("your_docs/", glob="**/*.pdf")
documents = loader.load()
chunks = split_documents(documents, chunk_size=500, chunk_overlap=50)
```

### Adjust Embedding Model
```python
embedding_manager = EmbeddingManager(model_name="all-mpnet-base-v2")
```

### Fine-tune LLM Parameters
```python
llm = ChatGroq(
    api_key=groq_api_key, 
    model="gemma2-9b-it", 
    temperature=0.1,  # Lower = more deterministic
    max_tokens=1024    # Maximum response length
)
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ‘¤ Author

**Rohan**
- GitHub: [@PhoneixDeadeye](https://github.com/PhoneixDeadeye)

## ğŸ™ Acknowledgments

- [LangChain](https://github.com/langchain-ai/langchain) - Framework for LLM applications
- [ChromaDB](https://www.trychroma.com/) - Vector database
- [SentenceTransformers](https://www.sbert.net/) - Embedding models
- [Groq](https://groq.com/) - Fast LLM inference

## ğŸ“š Resources

- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [RAG Paper](https://arxiv.org/abs/2005.11401)
- [Sentence-BERT Paper](https://arxiv.org/abs/1908.10084)

## ğŸ› Known Issues

- Large PDF files (>100 pages) may take longer to process
- Groq API has rate limits on free tier

## ğŸ—ºï¸ Roadmap

- [ ] Add support for more document formats (DOCX, HTML)
- [ ] Implement streaming responses
- [ ] Add web interface with Streamlit/Gradio
- [ ] Multi-language support
- [ ] Query optimization and caching
- [ ] Batch processing for large document sets
- [ ] Advanced citation and source linking

## ğŸ“ Support

If you encounter any issues or have questions, please file an issue on GitHub.

---

â­ If you find this project helpful, please give it a star!
